import os 
from flask import Flask, session, render_template, request, jsonify

app = Flask(__name__)
app.secret_key = "Ok"

# Variables for LLM integration
# llmInputs: List to store all requirement inputs from the frontend
# llmOutputs: String to store the LLM's response text
# llmLinks: List to store reference links from the LLM's response
llmInputs = []

llmOutputs = """
This frontend template is designed to work seamlessly with an LLM backend. The interface consists of a dynamic input system where users can add multiple requirement text boxes using the "+" button, and a processing system that displays both text output and reference links. The template uses Flask as the web framework and includes a dark/light mode toggle for better user experience. All user inputs are collected in the llmInputs list, which can be directly passed to your LLM backend for processing.

To connect with your LLM backend, you'll need to modify the /process endpoint in app.py. First, uncomment the line llmInputs = requirements to store the user inputs. Then, replace the current llmOutputs and llmLinks variables with your LLM's actual output. The frontend is already set up to handle the response format: it expects a JSON object with an output field containing the text response and a references field containing an array of URLs. The textarea will automatically adjust its height to display all content, and reference links will be displayed in light blue in dark mode.

The template includes comprehensive error handling and debugging features. Console logging is implemented throughout the process, making it easy to track the flow of data between the frontend and backend. When implementing your LLM backend, you can use these debug logs to verify that data is being passed correctly. The interface will display appropriate error messages if something goes wrong, and all text areas will automatically resize to show their full content. This makes it easy to integrate with any LLM backend while maintaining a clean, user-friendly interface.

The three main variables (llmInputs, llmOutputs, and llmLinks) work together to create a complete data flow. llmInputs collects all user requirements, which you can then process with your LLM. The processed results should be stored in llmOutputs as a string, and any reference links generated by the LLM should be stored in llmLinks as a list of URLs. The frontend will automatically handle the display of both the output text and reference links, with proper formatting and styling based on the current theme (light or dark mode).

To implement your LLM backend, simply modify the /process endpoint to handle the data flow: collect inputs from llmInputs, process them with your LLM, store the results in llmOutputs and llmLinks, and return them in the response. The frontend will handle the rest, including displaying the results, managing the theme, and providing a smooth user experience. The template is designed to be flexible and can work with any LLM backend that follows this simple data structure."""

llmLinks = ["https://example.com/link1", "https://example.com/link2"]  # Test links, will be replaced with actual LLM links

@app.route('/', methods=['GET'])
def index():
  return render_template(
    "index.html",
    title = "Welcome to Project 7: Research Helper",
  )

@app.route('/process', methods=['POST'])
def process():
    try:
        data = request.get_json()
        print("Received data:", data)  # Debug print
        
        if not data or 'requirements' not in data:
            return jsonify({
                'output': 'Error: Invalid request format',
                'references': []
            }), 400

        requirements = data.get('requirements', [])
        print("Requirements:", requirements)  # Debug print
        
        # Store inputs for LLM processing
        # TODO: Connect with LLM backend
        # llmInputs = requirements  # Uncomment when connecting to LLM backend
        
        # Use the predefined output and links
        output = llmOutputs
        references = llmLinks
        
        response = {
            'output': output,
            'references': references
        }
        print("Sending response:", response)  # Debug print
        
        return jsonify(response)
    except Exception as e:
        print("Error:", str(e))  # Debug print
        return jsonify({
            'output': f'Error processing request: {str(e)}',
            'references': []
        }), 500

if __name__ == '__main__':
  app.run(debug=True)
  
# Run in VSCode terminal:
# ./run.sh
